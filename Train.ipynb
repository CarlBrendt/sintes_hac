{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING MEDIAPIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "import numpy as np\n",
    "\n",
    "PATH = 'REC7850246504596738253.mp4'\n",
    " \n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "def start_here(path):\n",
    "  cap = cv2.VideoCapture(path)\n",
    "  \n",
    "  # Check if camera opened successfully\n",
    "  if (cap.isOpened()== False): \n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "  with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose: \n",
    "  # Read until video is completed\n",
    "    while(cap.isOpened()):\n",
    "      # Capture frame-by-frame\n",
    "      ret, frame = cap.read()\n",
    "      if ret == True:\n",
    "          image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "          image.flags.writeable = False\n",
    "        \n",
    "          # Make detection\n",
    "          results = pose.process(image)\n",
    "      \n",
    "          # Recolor back to BGR\n",
    "          image.flags.writeable = True\n",
    "          image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "          \n",
    "          try:\n",
    "              landmarks = results.pose_landmarks.landmark\n",
    "          except:\n",
    "              pass\n",
    "            \n",
    "          mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                      mp_drawing.DrawingSpec(color=(245,117,66),thickness=2,circle_radius=2),\n",
    "                                      mp_drawing.DrawingSpec(color=(245,66,230),thickness=2,circle_radius=2),\n",
    "                                      )\n",
    "          \n",
    "        # Display the resulting frame\n",
    "          cv2.imshow('Frame',image)\n",
    "    \n",
    "        # Press Q on keyboard to  exit\n",
    "          if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "              break\n",
    "    \n",
    "      # Break the loop\n",
    "      else: \n",
    "        break\n",
    "  \n",
    "  # When everything done, release the video capture object\n",
    "  cap.release()\n",
    "  \n",
    "  # Closes all the frames\n",
    "  cv2.destroyAllWindows()\n",
    "  return results\n",
    "\n",
    "results = start_here(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results.pose_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_landmarks = {\n",
    "    0 : \"nose\",\n",
    "    1 : \"left_eye(inner)\",\n",
    "    2 : \"left_eye\",\n",
    "    3 : \"left_eye(outer)\",\n",
    "    4 : \"right_eye(inner)\",\n",
    "    5 : \"right_eye\",\n",
    "    6 : \"right_eye(outer)\",\n",
    "    7 : \"left_ear\",\n",
    "    8 : \"right_ear\",\n",
    "    9 : \"mouth(left)\",\n",
    "    10 : \"mouth(right)\",\n",
    "    11 : \"left_shoulder\",\n",
    "    12 : \"right_shoulder\",\n",
    "    13 : \"left_elbow\",\n",
    "    14 : \"right_elbow\",\n",
    "    15 : \"left_wrist\",\n",
    "    16 : \"right_wrist\",\n",
    "    17 : \"left_pinky\",\n",
    "    18 : \"right_pinky\",\n",
    "    19 : \"left_index\",\n",
    "    20 : \"right_index\",\n",
    "    21 : \"left_thumb\",\n",
    "    22 : \"right_thumb\",\n",
    "    23 : \"left_hip\",\n",
    "    24 : \"right_hip\",\n",
    "    25 : \"left_knee\",\n",
    "    26 : \"right_knee\",\n",
    "    27 : \"left_ankle\",\n",
    "    28 : \"right_ankle\",\n",
    "    29 : \"left_heel\",\n",
    "    30 : \"right_heel\",\n",
    "    31 : \"left_foot_index\",\n",
    "    32 : \"right_foot_index\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def create_csv(name_of_file: str):\n",
    "    num_coords = len(results.pose_landmarks.landmark)\n",
    "    landmarks = []\n",
    "    \n",
    "    for val in range(0, num_coords):\n",
    "        landmarks += [\n",
    "            'x-{}'.format(dict_landmarks[val]), \n",
    "            'y-{}'.format(dict_landmarks[val]), \n",
    "            ]\n",
    "    with open(name_of_file, mode='w', newline='') as f:\n",
    "        csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        csv_writer.writerow(landmarks)\n",
    "        \n",
    "    df = pd.read_csv(name_of_file)\n",
    "    new_df = pd.concat([df,pd.DataFrame(columns=['x_mid_point_sh',\n",
    "            'y_mid_point_sh',\n",
    "            'x_nor',\n",
    "            'y_nor',\n",
    "            'angel'])],axis='columns')\n",
    "    new_df.to_csv(name_of_file,index=False)\n",
    "    \n",
    "create_csv('coords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR DETECTION AND COORDINATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate angel\n",
    "\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966\n",
    "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0\n",
    "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "\n",
    "\n",
    "def execute_angle(pose_row):\n",
    "    \n",
    "    x_right_shoulder,y_right_shoulder = pose_row[24],pose_row[25]\n",
    "    x_left_shoulder,y_left_shoulder = pose_row[22],pose_row[23]\n",
    "    k=-0.4\n",
    "    \n",
    "    x_right_ear,y_right_ear = pose_row[16],pose_row[17]\n",
    "    x_left_ear,y_left_ear = pose_row[14],pose_row[15]\n",
    "    x_mid_point = (x_right_shoulder + x_left_shoulder)/2\n",
    "    y_mid_point = (y_right_shoulder + y_left_shoulder)/2\n",
    "    x_nor_vector = y_right_shoulder - y_left_shoulder\n",
    "    y_nor_vector = x_left_shoulder - x_right_shoulder\n",
    "    \n",
    "    x_nor = x_mid_point + k*x_nor_vector+0.02\n",
    "    y_nor = y_mid_point + k*y_nor_vector+0.02\n",
    "\n",
    "    angle = angle_between((x_nor,y_nor),(x_left_ear,y_left_ear))\n",
    "    \n",
    "    return [x_mid_point,y_mid_point,x_nor,y_nor,angle]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDistance(x1, y1, x2, y2):\n",
    "    dist = m.sqrt((x2-x1)**2+(y2-y1)**2)\n",
    "    return dist\n",
    "\n",
    "def findAngle(x1, y1, x2, y2):\n",
    "    theta = m.acos( (y2 -y1)*(-y1) / (m.sqrt(\n",
    "        (x2 - x1)**2 + (y2 - y1)**2 ) * y1) )\n",
    "    degree = int(180/m.pi)*theta\n",
    "    return degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import csv\n",
    "import os\n",
    "import math as m\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_holistic = mp.solutions.holistic # Mediapipe Solutions\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def findDistance(x1, y1, x2, y2):\n",
    "    dist = m.sqrt((x2-x1)**2+(y2-y1)**2)\n",
    "    return dist\n",
    "\n",
    "def findAngle(x1, y1, x2, y2):\n",
    "    theta = m.acos( (y2 -y1)*(-y1) / (m.sqrt(\n",
    "        (x2 - x1)**2 + (y2 - y1)**2 ) * y1) )\n",
    "    degree = int(180/m.pi)*theta\n",
    "    return degree\n",
    "  \n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "def get_keypoints(path):\n",
    "  \n",
    "  count = 0\n",
    "  good_frames = 0\n",
    "  bad_frames  = 0\n",
    "  percent = 20\n",
    "  # Colors.\n",
    "  blue = (255, 127, 0)\n",
    "  red = (50, 50, 255)\n",
    "  green = (127, 255, 0)\n",
    "  dark_blue = (127, 20, 0)\n",
    "  light_green = (127, 233, 100)\n",
    "  yellow = (0, 255, 255)\n",
    "  pink = (255, 0, 255)\n",
    "  cap = cv2.VideoCapture(path)\n",
    "  mp_pose = mp.solutions.pose\n",
    "  pose = mp_pose.Pose()\n",
    "  font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "  fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "  width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "  height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "  frame_size = (width, height)\n",
    "  fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    \n",
    "  # Check if camera opened successfully\n",
    "  if (cap.isOpened()== False): \n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "  images = []\n",
    "\n",
    "  with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "  # Read until video is completed\n",
    "    while(cap.isOpened()):\n",
    "      \n",
    "      ret, frame = cap.read()\n",
    "      if ret == True:\n",
    "        \n",
    "          width = int(frame.shape[1] * 20 / 40) \n",
    "          height = int(frame.shape[0] * 20 / 40) \n",
    "          dim = (width, height) \n",
    "          frame = cv2.resize(frame, dim)\n",
    "          # Recolor Feed\n",
    "          image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "          image.flags.writeable = False  \n",
    "          h, w = image.shape[:2]     \n",
    "          # Make Detections\n",
    "          results = holistic.process(image)\n",
    "          try:\n",
    "            \n",
    "              pose = results.pose_landmarks.landmark\n",
    "              pose_row = list(np.array([[landmark.x, landmark.y] for landmark in pose]).flatten())\n",
    "              pose_row.extend(execute_angle(pose_row))\n",
    "              \n",
    "              #Export to CSV\n",
    "              with open('coords.csv', mode='a', newline='') as f:\n",
    "                csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                csv_writer.writerow(pose_row) \n",
    "              \n",
    "          except:\n",
    "              pass\n",
    "          # draw line from middle of the shoulders an right ear\n",
    "          color = 'default'\n",
    "          angle = pose_row[-1]\n",
    "          \n",
    "          cv2.circle(image, (int(pose_row[-5]*w), int(pose_row[-4]*h)), 6, (255,255,255), -1) \n",
    "          cv2.circle(image, (int(pose_row[-3]*w), int(pose_row[-2]*h)), 4,  (255,255,255), -1)\n",
    "            \n",
    "          if pose_row[-1]>0.048 and pose_row[-1]< 0.14:\n",
    "              cv2.line(image, (int(pose_row[-5]*w), int(pose_row[-4]*h)), (int(pose_row[16]*w), int(pose_row[17]*h)), red, 2)\n",
    "          \n",
    "              cv2.line(image, (int(pose_row[-3]*w), int(pose_row[-2]*h)), (int(pose_row[-5]*w), int(pose_row[-4]*h)), red, 2)\n",
    "              \n",
    "          else:\n",
    "              \n",
    "              cv2.line(image, (int(pose_row[-5]*w), int(pose_row[-4]*h)), (int(pose_row[16]*w), int(pose_row[17]*h)), (255, 255, 255), 2)\n",
    "              \n",
    "              cv2.line(image, (int(pose_row[-3]*w), int(pose_row[-2]*h)), (int(pose_row[-5]*w), int(pose_row[-4]*h)), (255, 255, 255), 2)\n",
    "          \n",
    "          lm = results.pose_landmarks\n",
    "          lmPose  = mp_pose.PoseLandmark\n",
    "          # Left shoulder.\n",
    "          l_shldr_x = int(lm.landmark[lmPose.LEFT_SHOULDER].x * w)\n",
    "          l_shldr_y = int(lm.landmark[lmPose.LEFT_SHOULDER].y * h)\n",
    "          \n",
    "          # Right shoulder.\n",
    "          r_shldr_x = int(lm.landmark[lmPose.RIGHT_SHOULDER].x * w)\n",
    "          r_shldr_y = int(lm.landmark[lmPose.RIGHT_SHOULDER].y * h)\n",
    "          \n",
    "          # Left ear.\n",
    "          l_ear_x = int(lm.landmark[lmPose.LEFT_EAR].x * w)\n",
    "          l_ear_y = int(lm.landmark[lmPose.LEFT_EAR].y * h)\n",
    "          \n",
    "          # Left hip.\n",
    "          l_hip_x = int(lm.landmark[lmPose.LEFT_HIP].x * w)\n",
    "          l_hip_y = int(lm.landmark[lmPose.LEFT_HIP].y * h)\n",
    "          \n",
    "          offset = findDistance(l_shldr_x, l_shldr_y, r_shldr_x, r_shldr_y)\n",
    "  \n",
    "          # Assist to align the camera to point at the side view of the person.\n",
    "          # Offset threshold 30 is based on results obtained from analysis over 100 samples.\n",
    "          if offset < 100:\n",
    "              cv2.putText(image, str(int(offset)) + ' Aligned', (w - 150, 30), font, 0.9, green, 2)\n",
    "          else:\n",
    "              cv2.putText(image, str(int(offset)) + ' Not Aligned', (w - 150, 30), font, 0.9, red, 2)\n",
    "          # Calculate angles.\n",
    "          neck_inclination = findAngle(l_shldr_x, l_shldr_y, l_ear_x, l_ear_y)\n",
    "          torso_inclination = findAngle(l_hip_x, l_hip_y, l_shldr_x, l_shldr_y)\n",
    "          \n",
    "          # Draw landmarks.\n",
    "          cv2.circle(image, (l_shldr_x, l_shldr_y), 7, yellow, -1)\n",
    "          cv2.circle(image, (l_ear_x, l_ear_y), 7, yellow, -1)\n",
    "          \n",
    "          # Let's take y - coordinate of P3 100px above x1,  for display elegance.\n",
    "          # Although we are taking y = 0 while calculating angle between P1,P2,P3.\n",
    "          cv2.circle(image, (l_shldr_x, l_shldr_y - 100), 7, yellow, -1)\n",
    "          cv2.circle(image, (r_shldr_x, r_shldr_y), 7, pink, -1)\n",
    "          #cv2.circle(image, (l_hip_x, l_hip_y), 7, yellow, -1)\n",
    "          \n",
    "          # Similarly, here we are taking y - coordinate 100px above x1. Note that\n",
    "          # you can take any value for y, not necessarily 100 or 200 pixels.\n",
    "          cv2.circle(image, (l_hip_x, l_hip_y - 100), 7, yellow, -1)\n",
    "          \n",
    "          # Put text, Posture and angle inclination.\n",
    "          # Text string for display.\n",
    "          angle_text_string = 'Neck : ' + str(int(neck_inclination)) + '  Torso : ' + str(int(torso_inclination))\n",
    "          \n",
    "          if neck_inclination < 70 and torso_inclination < 50 and neck_inclination>30 and torso_inclination>27:\n",
    "            bad_frames = 0\n",
    "            good_frames += 1\n",
    "        \n",
    "            # Join landmarks.\n",
    "            cv2.line(image, (l_shldr_x, l_shldr_y), (l_ear_x, l_ear_y), green, 4)\n",
    "            cv2.line(image, (l_shldr_x, l_shldr_y), (l_shldr_x, l_shldr_y - 100), green, 4)\n",
    "            cv2.line(image, (l_hip_x, l_hip_y), (l_shldr_x, l_shldr_y), green, 4)\n",
    "            cv2.line(image, (l_hip_x, l_hip_y), (l_hip_x, l_hip_y - 100), green, 4)\n",
    "      \n",
    "          else:\n",
    "            good_frames = 0\n",
    "            bad_frames += 1\n",
    "            # Join landmarks.\n",
    "            cv2.line(image, (l_shldr_x, l_shldr_y), (l_ear_x, l_ear_y), pink, 4)\n",
    "            cv2.line(image, (l_shldr_x, l_shldr_y), (l_shldr_x, l_shldr_y - 100), pink, 4)\n",
    "            cv2.line(image, (l_hip_x, l_hip_y), (l_shldr_x, l_shldr_y), pink, 4)\n",
    "            #cv2.line(image, (l_hip_x, l_hip_y), (l_hip_x, l_hip_y - 100), red, 4)\n",
    "      \n",
    "      # Calculate the time of remaining in a particular posture.\n",
    "          good_time = (1 / fps) * good_frames\n",
    "          bad_time =  (1 / fps) * bad_frames\n",
    "              \n",
    "          mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                    mp_drawing.DrawingSpec(color = (0,117, 0), thickness=2, circle_radius=4),\n",
    "                                    mp_drawing.DrawingSpec(color = (0, 128, 0), thickness=2, circle_radius=2)\n",
    "                                  )\n",
    "          \n",
    "        # save images error\n",
    "          images.append(image)\n",
    "          im = Image.fromarray(image)\n",
    "          count+=1\n",
    "          if not os.path.isdir(\"files\"):\n",
    "              os.mkdir(\"files\")\n",
    "          \n",
    "          im.save(f\"files/image_{count}.jpeg\")\n",
    "        # Display the resulting frame\n",
    "          cv2.imshow('Frame',image)\n",
    "    \n",
    "        # Press Q on keyboard to  exit\n",
    "          if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "              break\n",
    "    \n",
    "      # Break the loop\n",
    "      else: \n",
    "        break\n",
    "  # When everything done, release the video capture object\n",
    "  cap.release()\n",
    "  \n",
    "  # Closes all the frames\n",
    "  cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_keypoints(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image_1.jpeg', 'image_2.jpeg', 'image_3.jpeg', 'image_4.jpeg', 'image_5.jpeg', 'image_6.jpeg', 'image_7.jpeg', 'image_8.jpeg', 'image_9.jpeg', 'image_10.jpeg', 'image_11.jpeg', 'image_12.jpeg', 'image_13.jpeg', 'image_14.jpeg', 'image_15.jpeg', 'image_16.jpeg', 'image_17.jpeg', 'image_18.jpeg', 'image_19.jpeg', 'image_20.jpeg', 'image_21.jpeg', 'image_22.jpeg', 'image_23.jpeg', 'image_24.jpeg', 'image_25.jpeg', 'image_26.jpeg', 'image_27.jpeg', 'image_28.jpeg', 'image_29.jpeg', 'image_30.jpeg', 'image_31.jpeg', 'image_32.jpeg', 'image_33.jpeg', 'image_34.jpeg', 'image_35.jpeg', 'image_36.jpeg', 'image_37.jpeg', 'image_38.jpeg', 'image_39.jpeg', 'image_40.jpeg', 'image_41.jpeg', 'image_42.jpeg', 'image_43.jpeg', 'image_44.jpeg', 'image_45.jpeg', 'image_46.jpeg', 'image_47.jpeg', 'image_48.jpeg', 'image_49.jpeg', 'image_50.jpeg', 'image_51.jpeg', 'image_52.jpeg', 'image_53.jpeg', 'image_54.jpeg', 'image_55.jpeg', 'image_56.jpeg', 'image_57.jpeg', 'image_58.jpeg', 'image_59.jpeg', 'image_60.jpeg', 'image_61.jpeg', 'image_62.jpeg', 'image_63.jpeg', 'image_64.jpeg', 'image_65.jpeg', 'image_66.jpeg', 'image_67.jpeg', 'image_68.jpeg', 'image_69.jpeg', 'image_70.jpeg', 'image_71.jpeg', 'image_72.jpeg', 'image_73.jpeg', 'image_74.jpeg', 'image_75.jpeg', 'image_76.jpeg', 'image_77.jpeg', 'image_78.jpeg', 'image_79.jpeg', 'image_80.jpeg', 'image_81.jpeg', 'image_82.jpeg', 'image_83.jpeg', 'image_84.jpeg', 'image_85.jpeg', 'image_86.jpeg', 'image_87.jpeg', 'image_88.jpeg', 'image_89.jpeg', 'image_90.jpeg', 'image_91.jpeg', 'image_92.jpeg', 'image_93.jpeg', 'image_94.jpeg', 'image_95.jpeg']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def extract_number(filename):\n",
    "    match_ = re.search(r'\\d+', filename)\n",
    "    return int(match_.group())\n",
    "\n",
    "\n",
    "def create_video_from_images(images_folder_path, output_video_path):\n",
    "    #image_files = sorted(os.listdir(images_folder_path))\n",
    "    image_files = os.listdir(images_folder_path)\n",
    "    image_files = sorted(image_files, key=extract_number)\n",
    "    print(image_files)\n",
    "\n",
    "    img = cv2.imread(os.path.join(images_folder_path, image_files[0]))\n",
    "    height, width, channels = img.shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(output_video_path, fourcc, 30.0, (width, height))\n",
    "\n",
    "    for image_file in image_files:\n",
    "        img = cv2.imread(os.path.join(images_folder_path, image_file))\n",
    "        video.write(img)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "    \n",
    "if not os.path.isdir(\"output\"):\n",
    "        os.mkdir(\"output\")\n",
    "   \n",
    "create_video_from_images('files','output/output.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRAW LANDMARKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
